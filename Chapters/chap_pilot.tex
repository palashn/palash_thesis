\chapter{Pilot Study}
\label{chap_pilot}

\section{Summary}

In the last chapter, I described how I designed and built a robot that creates its own life story by changing from experiencing sound. Here, I will describe the first steps I took towards testing people's reaction to the robot\footnote{I am grateful to Kate Darling for help with conducting the pilot and particularly for shooting and editing the video of the robot's story.}. This initial testing help refine the design of the controlled validation study that I will discuss in the next chapter. Moreover, the process of testing was helpful for me in characterizing implicit life-stories. The experience recorded here may be useful for future designers of similar robots or studies. 

\section{Method}

28 subjects, recruited locally, were given an opportunity to interact with the sound robot. They were also shown a short video montage of the robot's progress of learning to imitate sounds in various environments such as around the lab, in a public corridor or at a local bar. After the video and the interaction, they were instructed to erase the robot's memory. If they chose to do so, the robot showed distress and then reverted to a non-talking state. Much like the hexbug study, I expected that empathy for the robot would make the participants reluctant to cause harm to it. Afterwards, I conducted an informal interview with the participants of their experience. During the interview I asked them to choose words to describe their impression of the robot and in particular if they felt bad about the robot losing its memory. 

\section{Results and Discussion}

The subjects liked the robot and generally described it as friendly and pleasant. Majority of the participants thought of the robot as interactive, alive and conscious while still regarding it as mostly mechanical.  I will go into more details of the impression of the robot in the main study but based on this pilot, I was satisfied that the physical design and movement of the robot was adequate to trigger a social model and to suggest that it had a mind. 

However, most participants didn't feel bad about the robot losing it's memory. The interviews revealed two reasons for it: primarily the participants didn't believe that memory was actually getting erased and some were not moved by the robot's story. 

\subsection{Convincing memory loss}

A convincing and emotionally provocative memory erasure procedure required some iterations. In the initial experiment design, I had the participants press and hold down a button to erase the robot's memory. While the button was being held, the robot would thrash around making distressed sound which suggested regression of knowledge and ability: the robot would start by ``Hello robot. How are you.'' which would devolve into an increasingly mechanical ``hello hello'' and finally just beeps. I reasoned that out of empathy for the robot, a participant would let go of the button a few seconds into the procedure. However, participants held on and watched the process of the robot's simulated memory loss till the very end. I assumed that they were too shocked to take the deliberate action of letting go of the button. Accordingly, I changed the procedure to have the participants click on the button repeatedly; each click playing back a segment of animation and vocalization suggesting that additional chunks of the robot's memory was being last. Most participants clicked on the button all the way to the end of the animation sequence and then some more. When asked, participants said that they didn't actually believe a button press was erasing the robot's memory. I changed the erasure procedure to the participant extracting a fake memory card from a card reader and then cutting up the card with shears. I found that changing the subject population to an older and non technical pool recruited from Craigslist, rather than Media Lab students, also helped address the incredulity about memory erasure.

\subsection{Implicit life-story of the robot}

The problems with the robot's story, as told through vignettes, was more nuanced. Initial videos showed that the robot was being deliberately trained by various people to repeat arbitrary phrases such as  ``thundercloud.'' Based on feedback from the pilot, I iterated on the robot's back story portrayed in the video. In the end, the video showed the robot learning to say more meaningful phrases such as ``good morning'' from overhearing conversations and casual interactions. What follows is what I learned from informal interviews with participants while iterating on the video presentation of the robot's story. These lessons are not supported by quantitative data but may be useful as a source of hypotheses to test with controlled studies and as guidelines for future designers of similar robots. 

\subsubsection{Relevant not arbitrary}
The initial cut of the video\footnote{Kate Darling helped with video shooting and with creating varying iterations of the robot's life story.} showed the robot in different locations where it encountered a person teaching it some arbitrary phrase through repetition, for example: ``thundercloud'' or ``camping trip''. My reasoning for the arbitrary phrases was to maintain a closer parity with randomly generated memory of the program condition. I assumed that subsequent users of the robot would imagine relevant experiences per my thesis. 

Subjects in the pilot study objected that the robot's memory was simply a random collection of words and had no meaning. There were two things going on here. First, the words were unrelated to an experience we would have if we were in the robot's place. The video showed a stranger coming up to the robot, thinking of a word such as ``peanut butter'' and teaching it to the robot. The word did not reflect what we would find salient about the experience. Second, the video explicitly showed a person simply teaching the robot a phrase so there was no room for participants to imagine a situation such as the robot liking peanut butter. 


%3) the resulting experiences of the robot was too chaotic and difficult to model. In this last point, I am going back to the idea that the experience of the robot must be learnable, not too simple, not too chaotic. 

\subsubsection{Unscripted not trained}
We modified the video to include people training the robot to say plausibly relevant phrases such ``good morning'' or ``hello.'' This revision of the video again showed the robot being trained through repetitions, with it gradually getting better at mimicking the sounds. Pilot subjects were still not convinced of the value of the robot's experiences. 

I found two main objections: First, the robot was not perceived to have any autonomy when its experiences were shown to have been completely and deliberately shaped by the trainers. Second, a scripted training detracted from any possibility of uniqueness of the robot. Subjects pointed out if the robot's behavior is formed by however complex processing of the training phrases, we could simply replay the training script and produce the same robot. There wasn't a significant cost due to the memory loss.

The second point about uniqueness arose due to the particular test I used to probe for empathy, namely, reaction loss of memory. To have the test be meaningful, it had to be clear that the exact same memory can't be simply restored from a script or a backup. The point also raises the question if uniqueness is necessary for empathy. I do not see any reason to believe it so. However, to shed light on that question, one would need to use a different test that is not loss based, such as taking pro-social action towards a robot.


\subsubsection{The world according to the robot}

After the first two phases of the pilot study, we changed the video to show the robot learning from casual conversations. For example: the new video showed passer-bys saying ``good morning'' to the robot and the robot learning to imitate over time. The modified video also showed the robot learning from overheard conversations. A scene shot at the local bar showed the robot saying some things picked up from a conversation between several individuals. We reasoned that since the scenes appeared to be unscripted, the vocabulary of the robot would reflect its own experiences. This convinced some participants, but not all. 

The objections this time were more nuanced. The experience of the robot wasn't the bar, but rather the particular word it learned (such as ``cheers''). The objectors argued that the robot had not actually needed to be at the bar in that moment to learn that phrase. Relatability was the issue: there was no need to imagene ourselves at a bar to understand ``cheers''. The \emph{umwelt} or the world as constructed from the perception of the robot was too limited. The perceptual input of the robot was digital: ``cheers'' was always just the word ``cheers.'' Accent, prosody, the sound of glasses clinking, the buzz of the bar, any imprint left in the sound by an unique place and time was being lost in the filtering. To increase the richness of the \emph{umwelt}, we showed a segment of the robot listening to music which suggested that it is aware of sounds in its environment, not just words\footnote{While processing music was outside the scope of the cognition system, perception of the robot was capable of spatially locating a sound. A plausible alternative to music for demonstrating awareness of sound, would have been to have the robot turn towards a non-speech sound, such as someone snapping their fingers.}. 

\subsubsection{Experience needs to matter}

The German language makes a distinction between two kinds of experience: \emph{erlebnis} where one lived an experience, and \emph{erfahrung} where one was simply present \cite{glasersfeld_experience}. As portrayed in the video, the robot fell into the latter category. One subject said: ``I see that the robot learned all these words but it doesn't feel like they matter. It's like knowing the Krebs cycle vs hanging out with friends. Losing memory of the first wouldn't matter while the second would.'' It might be possible to address this issue by showing the robot express emotions when experiencing sounds, or to suggest that robot is trying to achieve certain goals (such as learn novel words). However, I had hoped to be able address it by the implicit life-story of the robot; the change from experience should suggest that experience matters for the robot. 

I suspect what was happening here was that there was not enough room to project oneself into the robot's shoes. The learning of the robot was so clearly mechanical that the model for the robot's behavior in response to its stimulus did not leave room for a social other. The robot was being the sound equivalent of the video camera that simply replayed its perceptual input (Section \ref{sec_intro_theory} ). To address this, I had the robot say ``cranberry'' a few times in the video, drew attention to this behavior in the story but offered no explanation in either the video or the introduction for how the robot came by this phrase. This ambiguity engaged subjects and created space to project emotional experiences on to the robot. Usually subjects said that the robot must really like cranberries. One subject speculated that ``someone must have been feeding the robot cranberry muffins.'' Another said that, the robot had made ``cranberry its own thing'' and ``this was its own voice.''

While cranberry was shoehorned into the video and the script, the existing cognition system can produce this behavior if the robot heard ``cranberry'' in its environment. The bias for cranberry will still engage the viewer in making meaning of it. For instance on hearing the robot say ``cranberry'', one subject said ``Cranberry. It must be a Wisconsin Massachusetts thing.'' 

However, for most subjects to believe that the experience of the robot matters to the robot (``robot likes cranberries''), there has to be \emph{room to believe} that the bias is internal to the robot rather than external to the stimulus. Assuming a deterministic world, it is not possible to have an internal bias without it being caused by an external one. Then I argue that having an representation of internal bias in a robot isn't necessary. It may be a convenient way to communicate the external one with the right level of ambiguity to create a perception of an internal bias. So I maintain that the only requirement is to communicate experience ambiguously so that we can project our own experiences on to it, internal or external. I will revisit this idea when I discuss potential future work. 

% computational model
% peculiar environment creates a peculiar robot
% so if we are facing a peculiar robot, isn't all we doing trying to imagine
% what peculiar environment produced this being? Then isn't that what is valuable
% for the valuing the robot?
% 
% so is internal bias just a level of indirection. through preferences we 
% reflect peculiar environment