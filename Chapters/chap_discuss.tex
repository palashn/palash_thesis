
\chapter{Discussion}

%TODO some intro

\section{Implicit life-story of robots}

The validation study results showed that people felt worse if a robot with a experience story (the life-story condition) lost its memory compared to a programmed robot (the program condition). There was a wide range of responses to the robot losing its memory. Why asked why they felt bad about the robot's memory loss, subjects in the life-story condition responded that:

\begin{quotation}
BT5\footnote{TODO: the labels are conditions and not subject id. I will change this.}: ``It is almost like a child learning.''

BT5: ``The robot seemed to enjoy the people he met.''

BT5: ``The robot started with nothing [and] build those memories, those favorite words. It learned, that was all it knew, its world. ''

BT5: ``I don't know, I mean, humans are just programming after all like a robot too... and I would feel bad too about a human losing all of their memories.''

BI4: ``It felt like the death of a character.''

BT5: ``Because it learned from its experiences and those experiences are unique. They probably can't be experienced again, at least not in the same way.''

BT5: ``I felt connected to the robot.''

BT5: ``Because all those memories will be lost, like tears in rain.''
\end{quotation}

Some participants in the life-story condition were not moved by the prospect of the robot losing it's memory. One noted: ``I do not care about the stupid robot.'' Others suggested that since it was a machine with the capacity for relearning, it could just relearn what it was losing.

Perhaps unsurprisingly, the sentiment that the robot is simply a machine was seen more frequently in the program condition. Subjects said that they didn't feel bad for the robot beacuse ``it is just a robot'' or ``robots don't hvae emotions or feelings.'' Other responses included:

\begin{quotation}
AT3: ``I mean the robot is very cute so it almost reminds me of Wall-E... so I would feel bad if the robot lost anything but I must need to remember that this robot has no feelings.''

AT1: ``It doesn't really have any consciousness or sentence formation [ability], it just repeats stuff.''

AT1: ``The robot does not really have any personality, it seems very programmed. It does not seem like you are talking to \lq someon\rq, but just a randomized computer program.''

AT1: ``It's a robot. It's not a person.''
\end{quotation}


The themes that emerged for those who felt bad are that they see the robot as life-like, as a product of unique experiences, experiences that are meaningful to the robot (``favorite words'') and not unlike themselves (``connected to the robot''). This is in contrast to those in the program condition who thought of the robot more as a machine without capacity for feelings. Over the course of this chapter, I will delve more into these themes in discussion of the quantitative data as well as my findings from the informal pilots studies.


% BT5``After watching the video you feel a small connection to the robot. i would not like it if i lost my memories and the robot seemed to have some good ones so i would feel bad if it did!''




% BT1``It doesn't have conscious and to me it's just a computer.  I wouldn't feel bad if my computer lost it's memory.  It can always relearn again.''

% BT1


% BT5``It almost seemed lifelike.  It reminded me of my african grey parrot, Dante.''


% BT5``Because all those memories will be lost, like tears in rain. I mean it took a lot of effort from many different people to get the robot this far. It seems like such a waste to destroy all that work. Plus the robot reminds me of a real pet with it behavior. I can imagine people getting attached to it and might feel bad as a result.''

% BT1``it does not have a conscience, i would feel bad for the makers''

% 

% BT1``It's not my robot?''

\section{Differences between interaction and non-interaction}

The study also showed that across conditions, people felt worse after interacting with the robot and seeings its memory loss (B3 vs. B1) or (B2 vs. B1). However, the difference in B3 between the conditions decreased. One explanation for this decrease in difference is that physical interaction with the robot is contributing to empathy and masking the effect of the manipulation. In particular, the distress of the robot at memory erasure seemed to affect participants irrespective of the condition.  As described in the last chapter, when the robot's memory was erased it thrashed around making distressed sounds. Afterwards it tilted its head forward and became silent briefly. One participant from the program condition explained why they felt bad for the robot: ``The robot started jerking around almost like it was having a seizure. It was not unlike a human being being hurt in some way.'' Another participant, also in the program condition, felt bad because: ``robot looked and acted sad.'' 

These results mirror what I saw in the hexbug study where movement mitigated the effect of story manipulation (Section \ref{sec_hexbug_discussion} ). In that study, movement made people less hesitant to harm the robot, presumably due to the insect like motion of the hexbug. Based on prior work on robots with human stories \cite{gockley_valerie_roboreceptionist}, I speculate that once the novelty of physical interaction with a robot has worn off, the effect due to implicit life stories will be salient again, particularly if the robot continues to change over time. I will revisit this idea when I talk about potential future work.


\section{Empathy for the robot}


Empathy is the quality of feeling what others feel. So if people feel worse when a life-story robot is being harmed then it suggests that they feel empathy for it. In the earlier study, people empathized more with the hexbug, and were more reluctant to harm it, when it was given a life-story. Empathy was similarly demonstrated by subjects' responses in the sound robot study. When asked why they felt bad, one subject, in the life-story condition, said: ``After watching the video you feel a small connection to the robot. i would not like it if i lost my memories and the robot seemed to have some good ones so i would feel bad if it did!''

Just as in the hexbug study, my results in this study showed that across conditions, those with higher level of trait empathic concern (EC) felt worse (B3) about a robot being harmed compared to those with low EC. This further supports the argument that empathy had a role in people feeling bad for the robot, rather than their reaction just being due to destruction of value. 

%TODO reference someone else

When I examine B1 (how bad the subjects felt just after they watched the videos), the effect of the story manipulation was more pronounced on those with high trait empathy. This suggests that the life-story engenders empathy. While this result is similar to what we obtained in the hexbug study, in this study, in addition to empathic concern (EC), high level of fantasy (FS) and perspective taking (PT) trait empathies also significantly increased effect of stories on badness (B1) with the difference being greatest for FS. FS, to quote Davis ``taps respondents' tendencies to transpose themselves imaginatively into the feelings and actions of fictitious characters in books, movies, and plays,'' while PT is defined as ``the tendency to spontaneously adopt the psychological point of view of others'' \cite{davis_multidimensional_empathy}. When I had laid out my argument for why implicit life-stories could cause empathy, I had suggested that the mechanism at work might be that seeing a robot's experience ambiguously told through changes, engages us in imagining ourselves in its place, and we use our own experience to reconstruct its experience. The effects of FS and PT trait empathies on badness (B1) seen in the study are consistent with this hypothesized mechanism. 

Rosenthal-von der P{\"u}tten et al. found a correlation between subjects' FS scores and their responses to the robots being mistreated on video \cite{rosenthal_emotional_reaction}. However, across conditions, I did not find a significant difference in badness (B1) for subjects with high FS compared to those with low FS. Rather the data shows, for those with high FS, there is a significant difference in B1 between life-story and program. This suggests that effect I am seeing is not peculiar to video based empathy induction, but is related to the story manipulation. 


\section{Effect on empathy for others}

I had hypothesized that empathy for a robot would have an impact on empathy for another person. This study's data showed that if observing the robot suffering harm had no impact on how bad subjects felt (B3), then high trait empathic concern (EC) subjects would help another person, while low EC subjects would be less likely to. This behavior is consistent with the definition of empathic concern. 

However, the data showed if low EC individuals \emph{were} affected by the robot and felt bad, they would subsequently be more likely to help another in distress. This result is consistent with a study by Barnett et al that found that subjects when given an empathy arousing stimulus, for instance seeing video of crippled children suffering, would transfer the empathy to an unrelated target group and show pro-social behavior towards the target \cite{barnett_empathy_transfer}. 
A counterintuitive result from this study is that high EC individuals were less likely to help another person if they felt bad for the robot compared when they didn't feel bad.  In human-human interactions, the act of being empathic is known to reduce the capacity for empathy afterwards \cite{figley_compassion_fatigue_therapists}. It could be that I observed a similar effect in human-robot interaction for high empathic individuals. 

%Empathic reaction to another's suffering has been known to lead to compassion fatigue in human-human interactions where the empathizer has reduced capacity for empathy afterwards  
%% pen pickup

\section{Participant's impressions of the robot}

Participant's perceptions of the robot on the measures of animacy, anthropomorphism, likeability and intelligence were higher in the life-story condition than in the program condition. 

In the life-story condition, participants often compared the robot to a pet: ``It almost seemed lifelike.  It reminded me of my African grey parrot, Dante.'' While perceived experience made the robot seem more life-like and alive (see Figure \ref{fig_study_godspeed_madness}), overall the robot was considered to be mechanical rather than organic. The seeming contradiction of seeing the robot as ``alive'' yet ``mechanical'' could be explained by the form of the robot. The design prominently featured a speaker for a face and exposed the internal workings.

Participant's perceptions of the robot relating to its ``likeability'' were somewhat unexpected. While life-story did improve likeability, in both conditions subjects considered the robot to be ``kind'', ``nice'', ``pleasant'' and ``friendly'' (Figure \ref{fig_study_godspeed_madness}). I suggest that the rounded and juvenile form of the robot and its gentle breathing motion may have contributed to the positive perception here. Moreover, the robot does not drive interaction; it speaks when spoken to and if it does speak while overhearing a conversation, it doesn't require attention.

%TODO explain why the passive behavior contributes

Participant's perceiving the robot as more intelligence when they were shown its life story could be due to the common association between ``intelligence'' and ``learning'', so that when presented with evidence that the robot had the ability to learn subjects saw it as more intelligen than when they were told that it was simply following a program - particularly when that program was explained to them.

Often, participants stayed after the experiment to talk with the experimenter about what they thought of the robot. Some subjects said that they talk to themselves when they were alone and thought that the robot would make a good listening companion. A few mentioned that they would want to watch TV with the robot and have the robot comment in or share the experience with them. Not all reactions were positive though: one subject was uncomfortable with the idea of a robot that could be listening to what they were saying and said that they would not want to have such a robot at home. However, in general there was interest in the idea of a sound robot as a companion. 

 
\section{Wrapping up}

I found, in support of my thesis, that people have greater empathy for a robot when they perceive it as a product of experiences.  People also found the robot with life-story to be more animate, anthropomorphic, likable and intelligent. Lastly, I found that empathy for robots can have an effect on empathy for people. In the process of designing the study, I also learned about the nuances of the properties of implicit life-stories.




% uniqueness

% experience must matter. erleibness vs erfahrung.
% room for someone to be home. cranberry




% FUTURE

% shared experience

% characterization of ambiguity

% automatically generating relevant story

% application: old age home

% it seems that the only way to answer the question what makes us human is to
% to be able to answer the question what makes us perceive something to be human
% we will get better at refining and answering the second question but something
% was lost at the formulation of the question

